{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version :  2.2.0\n",
      "Tensorflow Version :  1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Keras based Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense,Conv2D,MaxPool2D,Dropout,BatchNormalization,Activation,Flatten,merge\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "# To Grow GPU Usage dynamically when required\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "print(\"Keras Version : \",keras.__version__)\n",
    "print(\"Tensorflow Version : \",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist Data Preparation :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "X_train_fil=X_train[Y_train>=5]\n",
    "Y_train_fil=Y_train[Y_train>=5]\n",
    "\n",
    "X_test_fil=X_test[Y_test>=5]\n",
    "Y_test_fil=Y_test[Y_test>=5]\n",
    "\n",
    "# One Hot Encoding\n",
    "num_classes = len(np.unique(Y_train_fil))\n",
    "'''\n",
    "Classes : \n",
    "6 -> 1 0 0 0\n",
    "7 -> 0 1 0 0\n",
    "8 -> 0 0 1 0\n",
    "9 -> 0 0 0 1\n",
    "\n",
    "'''\n",
    "print(num_classes)\n",
    "start_class= np.min(Y_train_fil)\n",
    "Y_train_fil_cat= np.eye(num_classes)[Y_train_fil-start_class]\n",
    "Y_test_fil_cat=np.eye(num_classes)[Y_test_fil-start_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (29404, 28, 28) Trainning Label (29404, 5)\n",
      "Test Data (4861, 28, 28) Test Label (4861, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data\", X_train_fil.shape,\"Trainning Label\",Y_train_fil_cat.shape)\n",
    "print(\"Test Data\", X_test_fil.shape,\"Test Label\",Y_test_fil_cat.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVNH Data Preparation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathtoSVNH='..\\\\Datasets\\\\SVNH\\\\'\n",
    "SVNH_tr=loadmat(PathtoSVNH+'train_32x32.mat')\n",
    "SVNH_tst=loadmat(PathtoSVNH+'test_32x32.mat')\n",
    "SVNH_train=SVNH_tr['X']\n",
    "SVNH_train_label=SVNH_tr['y']\n",
    "SVNH_test=SVNH_tst['X']\n",
    "SVNH_test_label=SVNH_tst['y']\n",
    "\n",
    "# Cleaning up the data into suitable standard format\n",
    "\n",
    "SVNH_train=np.swapaxes(SVNH_train,0,3).swapaxes(2,3)\n",
    "SVNH_test=np.swapaxes(SVNH_test,0,3).swapaxes(2,3)\n",
    "\n",
    "SVNH_train_label = np.concatenate( SVNH_train_label, axis=0 )\n",
    "SVNH_test_label = np.concatenate( SVNH_test_label, axis=0 )\n",
    "\n",
    "#  '0' has label 10.\n",
    "SVNH_train_label[SVNH_train_label==10]=0\n",
    "SVNH_test_label[SVNH_test_label==10]=0\n",
    "\n",
    "del SVNH_tr,SVNH_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "SVNH_train_fil=SVNH_train[SVNH_train_label<5]\n",
    "SVNH_train_label_fil=SVNH_train_label[SVNH_train_label<5]\n",
    "SVNH_test_fil=SVNH_test[SVNH_test_label<5]\n",
    "SVNH_test_label_fil=SVNH_test_label[SVNH_test_label<5]\n",
    "# One Hot Encoding\n",
    "'''\n",
    "Classes : \n",
    "6 -> 1 0 0 0\n",
    "7 -> 0 1 0 0\n",
    "8 -> 0 0 1 0\n",
    "9 -> 0 0 0 1\n",
    "\n",
    "'''\n",
    "print(num_classes)\n",
    "start_class= np.min(SVNH_train_label_fil)\n",
    "\n",
    "num_classes = len(np.unique(SVNH_train_label_fil))\n",
    "SVNH_train_label_fil_cat= np.eye(num_classes)[SVNH_train_label_fil-start_class] # -1 to make the indexing count\n",
    "SVNH_test_label_fil_cat=np.eye(num_classes)[SVNH_test_label_fil-start_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize/Center Crop the image\n",
    "\n",
    "SVNH_train_fil=SVNH_train_fil[:,2:30,2:30,:]\n",
    "SVNH_test_fil=SVNH_test_fil[:,2:30,2:30,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (45349, 28, 28, 3) Trainning Label (45349, 5)\n",
      "Test Data (16397, 28, 28, 3) Test Label (16397, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data\", SVNH_train_fil.shape,\"Trainning Label\",SVNH_train_label_fil_cat.shape)\n",
    "print(\"Test Data\", SVNH_test_fil.shape,\"Test Label\",SVNH_test_label_fil_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Lenet Architecture and Perform Domain Transfer Learning\n",
    "# between SVNH 0-4 to MNIST 5-9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://world4jason.gitbooks.io/research-log/content/deepLearning/CNN/img/lenet.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://world4jason.gitbooks.io/research-log/content/deepLearning/CNN/img/lenet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 6)         456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 425       \n",
      "=================================================================\n",
      "Total params: 44,301\n",
      "Trainable params: 44,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Base_Model=Sequential()\n",
    "Base_Model.add(Conv2D(batch_input_shape=(None,28,28,3),kernel_size=[5,5],filters=6))\n",
    "Base_Model.add(MaxPool2D(pool_size=2, strides=2, padding='valid'))\n",
    "Base_Model.add(Conv2D(kernel_size=[5,5],filters=16))\n",
    "Base_Model.add(MaxPool2D(pool_size=2,strides=2,padding='valid'))\n",
    "Base_Model.add(Flatten())\n",
    "Base_Model.add(Dense(units=120,activation='sigmoid'))\n",
    "Base_Model.add(Dense(units=84,activation='sigmoid'))\n",
    "Base_Model.add(Dense(units=num_classes,activation='softmax'))\n",
    "\n",
    "\n",
    "Base_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=keras.optimizers.Adam(lr=0.0001, beta_1=0.75, beta_2=0.99, epsilon=None, decay=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Logging - Run only if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_Model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45349 samples, validate on 16397 samples\n",
      "Epoch 1/1\n",
      "45349/45349 [==============================] - 41s 903us/step - loss: 1.4843 - acc: 0.3740 - val_loss: 1.3743 - val_acc: 0.4834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a68c4a8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base_Model.fit(SVNH_train_fil, SVNH_train_label_fil_cat,batch_size=64,epochs=1,verbose=1,validation_data=(SVNH_test_fil, SVNH_test_label_fil_cat),callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six different methods : \n",
    "* (i) Target only: the model is trained on D2 from scratch; \n",
    "* (ii) Fine-tune: the model is pretrained on D1 and fine-tuned on D2;\n",
    "* (iii) Matching networks [70]: we first pretrain the model on D3, then use D2 as the support set in the matching networks; \n",
    "* (iv) Fine-tuned matching networks: same as baseline iii, except that for each k the model is fine-tuned on D2 with 5-way (k 􀀀 1)-shot learning: k 􀀀 1 examples in each class are randomly selected as the support set, and the last example in each class is used as the query set; \n",
    "* (v) Fine-tune + adversarial: in addition to baseline ii, the model is also trained on D1 and D3 with a domain adversarial loss; \n",
    "* (vi.) Full model: fine-tune the model with the proposed multi-layer domain adversarial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-tf",
   "language": "python",
   "name": "keras-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
